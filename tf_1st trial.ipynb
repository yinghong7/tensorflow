{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.genfromtxt ('C:/Users/z5023853/Google Drive/Separated dataset_ML/a1_TCtrain.csv', \n",
    "                    delimiter = ',', usecols = (4,5,6), skip_header=1)\n",
    "y = np.genfromtxt ('C:/Users/z5023853/Google Drive/Separated dataset_ML/a1_TCtrain.csv', \n",
    "                   delimiter =',', usecols = (7),skip_header=1)\n",
    "x_cv = np.genfromtxt ('C:/Users/z5023853/Google Drive/Separated dataset_ML/a1_TCcv.csv', \n",
    "                    delimiter = ',', usecols = (4,5,6), skip_header=1)\n",
    "y_cv = np.genfromtxt ('C:/Users/z5023853/Google Drive/Separated dataset_ML/a1_TCcv.csv', \n",
    "                   delimiter =',', usecols = (7),skip_header=1)\n",
    "test_dataset = np.genfromtxt ('C:/Users/z5023853/Google Drive/Separated dataset_ML/a1_TCcv.csv', \n",
    "                              delimiter = ',', usecols = (4,5,6,7), skip_header=1)\n",
    "train_dataset = np.genfromtxt ('C:/Users/z5023853/Google Drive/Separated dataset_ML/a1_TCtrain.csv', \n",
    "                              delimiter = ',', usecols = (4,5,6,7), skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.reshape(y, (54,1)).astype(np.float32)\n",
    "y_test = np.reshape (y_cv, (14,1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 3)\n",
      "(14, 3)\n",
      "(68, 3)\n",
      "(54,)\n",
      "(14,)\n"
     ]
    }
   ],
   "source": [
    "print (x.shape)\n",
    "print (x_cv.shape)\n",
    "print (x_whole.shape)\n",
    "print (y.shape)\n",
    "print (y_cv.shape)\n",
    "x_whole = np.vstack ((x, x_cv)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 20)\n",
      "(14, 20)\n",
      "(54, 1)\n",
      "(14, 1)\n",
      "(54, 4)\n",
      "(14, 4)\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder ()\n",
    "x_enc = enc.fit_transform (x_whole).toarray().astype(np.float32)\n",
    "x_train = x_enc [:54, :]\n",
    "x_test = x_enc [54:, :]\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)\n",
    "print (y_train.shape)\n",
    "print (y_test.shape)\n",
    "#print (x_train)\n",
    "#print (x_test)\n",
    "#train_dataset = np.hstack ((x_train, y)).astype(np.float32)\n",
    "#test_dataset = np.hstack ((x_test, y_cv)).astype(np.float32)\n",
    "print (train_dataset.shape)\n",
    "print (test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_nodes = 10\n",
    "num_labels = y_train.shape[1]\n",
    "batch_size = 100\n",
    "num_features = x_train.shape[1]\n",
    "learning_rate = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = [None, num_features])\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = [None, num_labels])\n",
    "    tf_test_dataset = tf.constant(x_test)\n",
    "  \n",
    "    # Weights and Biases\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([num_features, hidden_nodes]))\n",
    "    layer1_biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    layer2_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Three-Layer Network\n",
    "    def three_layer_network (data):\n",
    "        input_layer = tf.matmul(data, layer1_weights)\n",
    "        hidden = tf.nn.relu(input_layer + layer1_biases)\n",
    "        output_layer = tf.matmul(hidden, layer2_weights) + layer2_biases\n",
    "        return output_layer\n",
    "    \n",
    "    # Model Scores\n",
    "    model_scores = three_layer_network(tf_train_dataset)\n",
    "    \n",
    "    # Loss\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = model_scores, labels = tf_train_labels))\n",
    "  \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(model_scores)\n",
    "    test_prediction = tf.nn.softmax(three_layer_network(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    preds_correct_boolean =  np.argmax(predictions, 1) == np.argmax(labels, 1)\n",
    "    correct_predictions = np.sum(preds_correct_boolean)\n",
    "    accuracy = 100.0 * correct_predictions / predictions.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step: 0 1\n",
      "Minibatch loss at step: 1000 1\n",
      "Minibatch loss at step: 2000 1\n",
      "Minibatch loss at step: 3000 1\n",
      "Minibatch loss at step: 4000 1\n",
      "Minibatch loss at step: 5000 1\n",
      "Minibatch loss at step: 6000 1\n",
      "Minibatch loss at step: 7000 1\n",
      "Minibatch loss at step: 8000 1\n",
      "Minibatch loss at step: 9000 1\n",
      "Minibatch loss at step: 10000 1\n",
      "Test accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "#this is a mini batch gradient descent\n",
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "        minibatch_data = x_train[offset:(offset + batch_size), :]\n",
    "        minibatch_labels = y_train[offset:(offset + batch_size)]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : minibatch_data, tf_train_labels : minibatch_labels}\n",
    "        \n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict = feed_dict\n",
    "            )\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print (\"Minibatch loss at step: {0} {1}\".format (step, 1))\n",
    "    print ('Test accuracy: {0}%'.format(accuracy(test_prediction.eval(), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(20, 10) dtype=float32_ref>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_weights\n",
    "#print (layer2_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"xw_plus_b:0\", shape=(54, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (first_layer_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = x_train\n",
    "training_labels = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_nodes = 5\n",
    "num_labels = training_labels.shape[1]\n",
    "num_features = training_data.shape[1]\n",
    "learning_rate = .01\n",
    "reg_lambda = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weights and Bias Arrays, just like in Tensorflow\n",
    "layer1_weights_array = np.random.normal(0, 1, [num_features, hidden_nodes]) \n",
    "layer2_weights_array = np.random.normal(0, 1, [hidden_nodes, num_labels]) \n",
    "layer1_biases_array = np.zeros((1, hidden_nodes))\n",
    "layer2_biases_array = np.zeros((1, num_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Relu activation\n",
    "def relu_activation(data_array):\n",
    "    return np.maximum(data_array, 0)\n",
    "\n",
    "#Softmax activation\n",
    "def softmax(output_array):\n",
    "    logits_exp = np.exp(output_array)\n",
    "    return logits_exp / np.sum(logits_exp, axis = 1, keepdims = True)\n",
    "\n",
    "#softmax cross entropy\n",
    "def cross_entropy_softmax_loss_array(softmax_probs_array, y_onehot):\n",
    "    indices = np.argmax(y_onehot, axis = 1).astype(int)\n",
    "    predicted_probability = softmax_probs_array[np.arange(len(softmax_probs_array)), indices]\n",
    "    log_preds = np.log(predicted_probability)\n",
    "    loss = -1.0 * np.sum(log_preds) / len(log_preds)\n",
    "    return loss\n",
    "\n",
    "#Regularisation of softmax\n",
    "def regularization_L2_softmax_loss(reg_lambda, weight1, weight2):\n",
    "    weight1_loss = 0.5 * reg_lambda * np.sum(weight1 * weight1)\n",
    "    weight2_loss = 0.5 * reg_lambda * np.sum(weight2 * weight2)\n",
    "    return weight1_loss + weight2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\z5023853\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in maximum\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\z5023853\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in less_equal\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: nan\n",
      "Loss at step 500: nan\n",
      "Loss at step 1000: nan\n",
      "Loss at step 1500: nan\n",
      "Loss at step 2000: nan\n",
      "Loss at step 2500: nan\n",
      "Loss at step 3000: nan\n",
      "Loss at step 3500: nan\n",
      "Loss at step 4000: nan\n",
      "Loss at step 4500: nan\n",
      "Loss at step 5000: nan\n"
     ]
    }
   ],
   "source": [
    "for step in range(5001):\n",
    "\n",
    "    input_layer = np.dot(training_data, layer1_weights_array)\n",
    "    hidden_layer = relu_activation(input_layer + layer1_biases_array)\n",
    "    output_layer = np.dot(hidden_layer, layer2_weights_array) + layer2_biases_array\n",
    "    output_probs = softmax(output_layer)\n",
    "    \n",
    "    loss = cross_entropy_softmax_loss_array(output_probs, training_labels)\n",
    "    loss += regularization_L2_softmax_loss(reg_lambda, layer1_weights_array, layer2_weights_array)\n",
    "\n",
    "    output_error_signal = (output_probs - training_labels) / output_probs.shape[0]\n",
    "    \n",
    "    error_signal_hidden = np.dot(output_error_signal, layer2_weights_array.T) \n",
    "    error_signal_hidden[hidden_layer <= 0] = 0\n",
    "    \n",
    "    gradient_layer2_weights = np.dot(hidden_layer.T, output_error_signal)\n",
    "    gradient_layer2_bias = np.sum(output_error_signal, axis = 0, keepdims = True)\n",
    "    \n",
    "    gradient_layer1_weights = np.dot(training_data.T, error_signal_hidden)\n",
    "    gradient_layer1_bias = np.sum(error_signal_hidden, axis = 0, keepdims = True)\n",
    "\n",
    "    gradient_layer2_weights += reg_lambda * layer2_weights_array\n",
    "    gradient_layer1_weights += reg_lambda * layer1_weights_array\n",
    "\n",
    "    layer1_weights_array -= learning_rate * gradient_layer1_weights\n",
    "    layer1_biases_array -= learning_rate * gradient_layer1_bias\n",
    "    layer2_weights_array -= learning_rate * gradient_layer2_weights\n",
    "    layer2_biases_array -= learning_rate * gradient_layer2_bias\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "            print ('Loss at step {0}: {1}'.format(step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (14,21) and (20,5) not aligned: 21 (dim 1) != 20 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-d0512048d664>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer1_weights_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhidden_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlayer1_biases_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer2_weights_array\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlayer2_biases_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy: {0}%'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (14,21) and (20,5) not aligned: 21 (dim 1) != 20 (dim 0)"
     ]
    }
   ],
   "source": [
    "input_layer = np.dot(test_dataset, layer1_weights_array)\n",
    "hidden_layer = relu_activation(input_layer + layer1_biases_array)\n",
    "scores = np.dot(hidden_layer, layer2_weights_array) + layer2_biases_array\n",
    "probs = softmax(scores)\n",
    "print ('Test accuracy: {0}%'.format(accuracy(probs, test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
